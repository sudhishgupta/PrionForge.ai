{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.091533180778033,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04576659038901602,
      "grad_norm": 0.5106503963470459,
      "learning_rate": 4.986363636363637e-05,
      "loss": 9.9076,
      "step": 5
    },
    {
      "epoch": 0.09153318077803203,
      "grad_norm": 0.7180374264717102,
      "learning_rate": 4.9727272727272725e-05,
      "loss": 9.5307,
      "step": 10
    },
    {
      "epoch": 0.13729977116704806,
      "grad_norm": 0.6374176740646362,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 9.3767,
      "step": 15
    },
    {
      "epoch": 0.18306636155606407,
      "grad_norm": 1.0147337913513184,
      "learning_rate": 4.9272727272727276e-05,
      "loss": 9.8768,
      "step": 20
    },
    {
      "epoch": 0.2288329519450801,
      "grad_norm": 0.6760709285736084,
      "learning_rate": 4.904545454545455e-05,
      "loss": 9.7035,
      "step": 25
    },
    {
      "epoch": 0.2745995423340961,
      "grad_norm": 1.1101434230804443,
      "learning_rate": 4.881818181818182e-05,
      "loss": 10.0573,
      "step": 30
    },
    {
      "epoch": 0.32036613272311215,
      "grad_norm": 1.1157687902450562,
      "learning_rate": 4.859090909090909e-05,
      "loss": 9.4174,
      "step": 35
    },
    {
      "epoch": 0.36613272311212813,
      "grad_norm": 1.991768479347229,
      "learning_rate": 4.8363636363636364e-05,
      "loss": 9.4955,
      "step": 40
    },
    {
      "epoch": 0.41189931350114417,
      "grad_norm": 1.3249733448028564,
      "learning_rate": 4.813636363636364e-05,
      "loss": 9.7745,
      "step": 45
    },
    {
      "epoch": 0.4576659038901602,
      "grad_norm": 3.144136428833008,
      "learning_rate": 4.790909090909091e-05,
      "loss": 9.2001,
      "step": 50
    },
    {
      "epoch": 0.5034324942791762,
      "grad_norm": 2.548074245452881,
      "learning_rate": 4.768181818181818e-05,
      "loss": 9.3495,
      "step": 55
    },
    {
      "epoch": 0.5491990846681922,
      "grad_norm": 2.347759485244751,
      "learning_rate": 4.745454545454546e-05,
      "loss": 9.308,
      "step": 60
    },
    {
      "epoch": 0.5949656750572082,
      "grad_norm": 4.0222554206848145,
      "learning_rate": 4.722727272727273e-05,
      "loss": 9.3353,
      "step": 65
    },
    {
      "epoch": 0.6407322654462243,
      "grad_norm": 3.287247657775879,
      "learning_rate": 4.7e-05,
      "loss": 8.9224,
      "step": 70
    },
    {
      "epoch": 0.6864988558352403,
      "grad_norm": 3.6088969707489014,
      "learning_rate": 4.6772727272727276e-05,
      "loss": 8.7903,
      "step": 75
    },
    {
      "epoch": 0.7322654462242563,
      "grad_norm": 2.957824230194092,
      "learning_rate": 4.654545454545455e-05,
      "loss": 8.5254,
      "step": 80
    },
    {
      "epoch": 0.7780320366132724,
      "grad_norm": 1.313855528831482,
      "learning_rate": 4.631818181818182e-05,
      "loss": 8.6761,
      "step": 85
    },
    {
      "epoch": 0.8237986270022883,
      "grad_norm": 3.8809685707092285,
      "learning_rate": 4.609090909090909e-05,
      "loss": 8.6995,
      "step": 90
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 5.051982879638672,
      "learning_rate": 4.5863636363636365e-05,
      "loss": 8.4032,
      "step": 95
    },
    {
      "epoch": 0.9153318077803204,
      "grad_norm": 2.358912706375122,
      "learning_rate": 4.563636363636364e-05,
      "loss": 7.9584,
      "step": 100
    },
    {
      "epoch": 0.9610983981693364,
      "grad_norm": 2.72957706451416,
      "learning_rate": 4.540909090909091e-05,
      "loss": 8.2162,
      "step": 105
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.908499717712402,
      "learning_rate": 4.518181818181819e-05,
      "loss": 7.7761,
      "step": 110
    },
    {
      "epoch": 1.045766590389016,
      "grad_norm": 2.2751166820526123,
      "learning_rate": 4.495454545454545e-05,
      "loss": 7.8275,
      "step": 115
    },
    {
      "epoch": 1.091533180778032,
      "grad_norm": 3.1163597106933594,
      "learning_rate": 4.472727272727273e-05,
      "loss": 8.0083,
      "step": 120
    },
    {
      "epoch": 1.137299771167048,
      "grad_norm": 2.911372184753418,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 7.6191,
      "step": 125
    },
    {
      "epoch": 1.1830663615560641,
      "grad_norm": 1.2948646545410156,
      "learning_rate": 4.4272727272727276e-05,
      "loss": 7.3366,
      "step": 130
    },
    {
      "epoch": 1.22883295194508,
      "grad_norm": 2.6728246212005615,
      "learning_rate": 4.404545454545455e-05,
      "loss": 7.5745,
      "step": 135
    },
    {
      "epoch": 1.2745995423340961,
      "grad_norm": 3.009147882461548,
      "learning_rate": 4.381818181818182e-05,
      "loss": 7.4782,
      "step": 140
    },
    {
      "epoch": 1.3203661327231122,
      "grad_norm": 1.9699928760528564,
      "learning_rate": 4.359090909090909e-05,
      "loss": 7.1527,
      "step": 145
    },
    {
      "epoch": 1.366132723112128,
      "grad_norm": 2.509119749069214,
      "learning_rate": 4.3363636363636365e-05,
      "loss": 6.9832,
      "step": 150
    },
    {
      "epoch": 1.4118993135011442,
      "grad_norm": 4.067917823791504,
      "learning_rate": 4.313636363636364e-05,
      "loss": 6.9311,
      "step": 155
    },
    {
      "epoch": 1.4576659038901603,
      "grad_norm": 2.9995360374450684,
      "learning_rate": 4.290909090909091e-05,
      "loss": 7.1518,
      "step": 160
    },
    {
      "epoch": 1.5034324942791764,
      "grad_norm": 3.395786762237549,
      "learning_rate": 4.268181818181818e-05,
      "loss": 6.8617,
      "step": 165
    },
    {
      "epoch": 1.5491990846681922,
      "grad_norm": 1.7311134338378906,
      "learning_rate": 4.245454545454546e-05,
      "loss": 7.0172,
      "step": 170
    },
    {
      "epoch": 1.594965675057208,
      "grad_norm": 2.409484386444092,
      "learning_rate": 4.222727272727273e-05,
      "loss": 6.9074,
      "step": 175
    },
    {
      "epoch": 1.6407322654462244,
      "grad_norm": 4.165746212005615,
      "learning_rate": 4.2e-05,
      "loss": 6.959,
      "step": 180
    },
    {
      "epoch": 1.6864988558352403,
      "grad_norm": 1.6805182695388794,
      "learning_rate": 4.1772727272727277e-05,
      "loss": 6.6713,
      "step": 185
    },
    {
      "epoch": 1.7322654462242562,
      "grad_norm": 2.8741469383239746,
      "learning_rate": 4.154545454545455e-05,
      "loss": 6.2253,
      "step": 190
    },
    {
      "epoch": 1.7780320366132725,
      "grad_norm": 1.51035737991333,
      "learning_rate": 4.131818181818182e-05,
      "loss": 6.1787,
      "step": 195
    },
    {
      "epoch": 1.8237986270022883,
      "grad_norm": 1.8436945676803589,
      "learning_rate": 4.109090909090909e-05,
      "loss": 6.0181,
      "step": 200
    },
    {
      "epoch": 1.8695652173913042,
      "grad_norm": 1.302769660949707,
      "learning_rate": 4.0863636363636365e-05,
      "loss": 6.357,
      "step": 205
    },
    {
      "epoch": 1.9153318077803205,
      "grad_norm": 2.8370110988616943,
      "learning_rate": 4.063636363636364e-05,
      "loss": 6.4333,
      "step": 210
    },
    {
      "epoch": 1.9610983981693364,
      "grad_norm": 1.7100830078125,
      "learning_rate": 4.0409090909090916e-05,
      "loss": 5.5426,
      "step": 215
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.7450287342071533,
      "learning_rate": 4.018181818181818e-05,
      "loss": 6.392,
      "step": 220
    },
    {
      "epoch": 2.045766590389016,
      "grad_norm": 2.236514091491699,
      "learning_rate": 3.9954545454545454e-05,
      "loss": 6.1696,
      "step": 225
    },
    {
      "epoch": 2.091533180778032,
      "grad_norm": 2.093836784362793,
      "learning_rate": 3.972727272727273e-05,
      "loss": 5.7438,
      "step": 230
    },
    {
      "epoch": 2.137299771167048,
      "grad_norm": 1.5536959171295166,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 6.1996,
      "step": 235
    },
    {
      "epoch": 2.183066361556064,
      "grad_norm": 1.362420678138733,
      "learning_rate": 3.927272727272727e-05,
      "loss": 6.0448,
      "step": 240
    },
    {
      "epoch": 2.2288329519450802,
      "grad_norm": 0.8595212697982788,
      "learning_rate": 3.904545454545455e-05,
      "loss": 5.7138,
      "step": 245
    },
    {
      "epoch": 2.274599542334096,
      "grad_norm": 1.0401211977005005,
      "learning_rate": 3.881818181818182e-05,
      "loss": 5.3901,
      "step": 250
    },
    {
      "epoch": 2.320366132723112,
      "grad_norm": 1.0709565877914429,
      "learning_rate": 3.859090909090909e-05,
      "loss": 5.806,
      "step": 255
    },
    {
      "epoch": 2.3661327231121283,
      "grad_norm": 1.3653650283813477,
      "learning_rate": 3.8363636363636365e-05,
      "loss": 5.6749,
      "step": 260
    },
    {
      "epoch": 2.411899313501144,
      "grad_norm": 1.1599152088165283,
      "learning_rate": 3.813636363636364e-05,
      "loss": 5.3099,
      "step": 265
    },
    {
      "epoch": 2.45766590389016,
      "grad_norm": 0.7669359445571899,
      "learning_rate": 3.790909090909091e-05,
      "loss": 5.7816,
      "step": 270
    },
    {
      "epoch": 2.5034324942791764,
      "grad_norm": 2.1718194484710693,
      "learning_rate": 3.768181818181818e-05,
      "loss": 5.7213,
      "step": 275
    },
    {
      "epoch": 2.5491990846681922,
      "grad_norm": 1.3287687301635742,
      "learning_rate": 3.745454545454546e-05,
      "loss": 5.2676,
      "step": 280
    },
    {
      "epoch": 2.594965675057208,
      "grad_norm": 1.1546730995178223,
      "learning_rate": 3.7227272727272726e-05,
      "loss": 5.596,
      "step": 285
    },
    {
      "epoch": 2.6407322654462244,
      "grad_norm": 0.6851794719696045,
      "learning_rate": 3.7e-05,
      "loss": 5.2579,
      "step": 290
    },
    {
      "epoch": 2.6864988558352403,
      "grad_norm": 1.215840458869934,
      "learning_rate": 3.677272727272728e-05,
      "loss": 5.9214,
      "step": 295
    },
    {
      "epoch": 2.732265446224256,
      "grad_norm": 1.0551679134368896,
      "learning_rate": 3.654545454545455e-05,
      "loss": 5.9393,
      "step": 300
    },
    {
      "epoch": 2.7780320366132725,
      "grad_norm": 0.6812411546707153,
      "learning_rate": 3.6318181818181815e-05,
      "loss": 5.5856,
      "step": 305
    },
    {
      "epoch": 2.8237986270022883,
      "grad_norm": 0.5488830208778381,
      "learning_rate": 3.6090909090909093e-05,
      "loss": 6.0503,
      "step": 310
    },
    {
      "epoch": 2.869565217391304,
      "grad_norm": 0.8788467645645142,
      "learning_rate": 3.5863636363636366e-05,
      "loss": 5.3002,
      "step": 315
    },
    {
      "epoch": 2.9153318077803205,
      "grad_norm": 0.4101632535457611,
      "learning_rate": 3.563636363636364e-05,
      "loss": 4.9203,
      "step": 320
    },
    {
      "epoch": 2.9610983981693364,
      "grad_norm": 0.6949917078018188,
      "learning_rate": 3.540909090909091e-05,
      "loss": 5.0117,
      "step": 325
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.5083054304122925,
      "learning_rate": 3.518181818181818e-05,
      "loss": 5.4286,
      "step": 330
    },
    {
      "epoch": 3.045766590389016,
      "grad_norm": 0.746178925037384,
      "learning_rate": 3.4954545454545454e-05,
      "loss": 5.2307,
      "step": 335
    },
    {
      "epoch": 3.091533180778032,
      "grad_norm": 1.7407349348068237,
      "learning_rate": 3.472727272727273e-05,
      "loss": 5.0427,
      "step": 340
    },
    {
      "epoch": 3.137299771167048,
      "grad_norm": 0.47075602412223816,
      "learning_rate": 3.45e-05,
      "loss": 5.3381,
      "step": 345
    },
    {
      "epoch": 3.183066361556064,
      "grad_norm": 0.5597203373908997,
      "learning_rate": 3.427272727272727e-05,
      "loss": 5.0391,
      "step": 350
    },
    {
      "epoch": 3.2288329519450802,
      "grad_norm": 0.716749370098114,
      "learning_rate": 3.404545454545455e-05,
      "loss": 5.4797,
      "step": 355
    },
    {
      "epoch": 3.274599542334096,
      "grad_norm": 0.8171153664588928,
      "learning_rate": 3.381818181818182e-05,
      "loss": 4.8393,
      "step": 360
    },
    {
      "epoch": 3.320366132723112,
      "grad_norm": 1.2649186849594116,
      "learning_rate": 3.3590909090909094e-05,
      "loss": 5.1785,
      "step": 365
    },
    {
      "epoch": 3.3661327231121283,
      "grad_norm": 0.5617393255233765,
      "learning_rate": 3.3363636363636366e-05,
      "loss": 5.35,
      "step": 370
    },
    {
      "epoch": 3.411899313501144,
      "grad_norm": 0.48512858152389526,
      "learning_rate": 3.313636363636364e-05,
      "loss": 4.9438,
      "step": 375
    },
    {
      "epoch": 3.45766590389016,
      "grad_norm": 0.5768488049507141,
      "learning_rate": 3.290909090909091e-05,
      "loss": 6.0269,
      "step": 380
    },
    {
      "epoch": 3.5034324942791764,
      "grad_norm": 0.6237714886665344,
      "learning_rate": 3.268181818181819e-05,
      "loss": 5.748,
      "step": 385
    },
    {
      "epoch": 3.5491990846681922,
      "grad_norm": 0.5093242526054382,
      "learning_rate": 3.2454545454545454e-05,
      "loss": 5.5523,
      "step": 390
    },
    {
      "epoch": 3.594965675057208,
      "grad_norm": 0.3685077726840973,
      "learning_rate": 3.2227272727272726e-05,
      "loss": 5.6924,
      "step": 395
    },
    {
      "epoch": 3.6407322654462244,
      "grad_norm": 0.6855225563049316,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 5.0582,
      "step": 400
    },
    {
      "epoch": 3.6864988558352403,
      "grad_norm": 0.5398473739624023,
      "learning_rate": 3.177272727272728e-05,
      "loss": 4.9554,
      "step": 405
    },
    {
      "epoch": 3.732265446224256,
      "grad_norm": 0.4836277663707733,
      "learning_rate": 3.154545454545454e-05,
      "loss": 4.8915,
      "step": 410
    },
    {
      "epoch": 3.7780320366132725,
      "grad_norm": 0.5540454983711243,
      "learning_rate": 3.131818181818182e-05,
      "loss": 4.3142,
      "step": 415
    },
    {
      "epoch": 3.8237986270022883,
      "grad_norm": 0.432809442281723,
      "learning_rate": 3.1090909090909094e-05,
      "loss": 5.4213,
      "step": 420
    },
    {
      "epoch": 3.869565217391304,
      "grad_norm": 0.5151780247688293,
      "learning_rate": 3.0863636363636366e-05,
      "loss": 5.0334,
      "step": 425
    },
    {
      "epoch": 3.9153318077803205,
      "grad_norm": 0.9689455628395081,
      "learning_rate": 3.063636363636364e-05,
      "loss": 4.9801,
      "step": 430
    },
    {
      "epoch": 3.9610983981693364,
      "grad_norm": 0.7842549681663513,
      "learning_rate": 3.040909090909091e-05,
      "loss": 5.1461,
      "step": 435
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.6089579463005066,
      "learning_rate": 3.0181818181818182e-05,
      "loss": 4.845,
      "step": 440
    },
    {
      "epoch": 4.045766590389016,
      "grad_norm": 0.5883135199546814,
      "learning_rate": 2.9954545454545458e-05,
      "loss": 4.9641,
      "step": 445
    },
    {
      "epoch": 4.091533180778032,
      "grad_norm": 0.5436129570007324,
      "learning_rate": 2.972727272727273e-05,
      "loss": 4.33,
      "step": 450
    },
    {
      "epoch": 4.137299771167048,
      "grad_norm": 0.333438515663147,
      "learning_rate": 2.95e-05,
      "loss": 4.3886,
      "step": 455
    },
    {
      "epoch": 4.183066361556064,
      "grad_norm": 0.42630741000175476,
      "learning_rate": 2.9272727272727274e-05,
      "loss": 4.7281,
      "step": 460
    },
    {
      "epoch": 4.22883295194508,
      "grad_norm": 0.591124951839447,
      "learning_rate": 2.9045454545454546e-05,
      "loss": 5.0002,
      "step": 465
    },
    {
      "epoch": 4.274599542334096,
      "grad_norm": 0.5599943399429321,
      "learning_rate": 2.8818181818181822e-05,
      "loss": 5.7312,
      "step": 470
    },
    {
      "epoch": 4.320366132723112,
      "grad_norm": 0.3309471607208252,
      "learning_rate": 2.859090909090909e-05,
      "loss": 5.1536,
      "step": 475
    },
    {
      "epoch": 4.366132723112128,
      "grad_norm": 0.37468427419662476,
      "learning_rate": 2.8363636363636363e-05,
      "loss": 5.2967,
      "step": 480
    },
    {
      "epoch": 4.411899313501144,
      "grad_norm": 0.4831404983997345,
      "learning_rate": 2.813636363636364e-05,
      "loss": 5.2649,
      "step": 485
    },
    {
      "epoch": 4.4576659038901605,
      "grad_norm": 0.5194958448410034,
      "learning_rate": 2.7909090909090914e-05,
      "loss": 5.2096,
      "step": 490
    },
    {
      "epoch": 4.503432494279176,
      "grad_norm": 0.6167853474617004,
      "learning_rate": 2.7681818181818183e-05,
      "loss": 4.8101,
      "step": 495
    },
    {
      "epoch": 4.549199084668192,
      "grad_norm": 0.43075358867645264,
      "learning_rate": 2.7454545454545455e-05,
      "loss": 4.4786,
      "step": 500
    },
    {
      "epoch": 4.5949656750572085,
      "grad_norm": 0.5931667685508728,
      "learning_rate": 2.722727272727273e-05,
      "loss": 5.4421,
      "step": 505
    },
    {
      "epoch": 4.640732265446224,
      "grad_norm": 0.2584284842014313,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 4.881,
      "step": 510
    },
    {
      "epoch": 4.68649885583524,
      "grad_norm": 0.7040904760360718,
      "learning_rate": 2.677272727272727e-05,
      "loss": 5.2912,
      "step": 515
    },
    {
      "epoch": 4.732265446224257,
      "grad_norm": 0.3992418348789215,
      "learning_rate": 2.6545454545454547e-05,
      "loss": 5.1279,
      "step": 520
    },
    {
      "epoch": 4.778032036613272,
      "grad_norm": 0.5127118825912476,
      "learning_rate": 2.631818181818182e-05,
      "loss": 4.7976,
      "step": 525
    },
    {
      "epoch": 4.823798627002288,
      "grad_norm": 0.3255992829799652,
      "learning_rate": 2.6090909090909094e-05,
      "loss": 4.5043,
      "step": 530
    },
    {
      "epoch": 4.869565217391305,
      "grad_norm": 0.32275402545928955,
      "learning_rate": 2.5863636363636363e-05,
      "loss": 5.1118,
      "step": 535
    },
    {
      "epoch": 4.91533180778032,
      "grad_norm": 0.3562456965446472,
      "learning_rate": 2.5636363636363635e-05,
      "loss": 4.5945,
      "step": 540
    },
    {
      "epoch": 4.961098398169336,
      "grad_norm": 0.4410160779953003,
      "learning_rate": 2.540909090909091e-05,
      "loss": 4.9037,
      "step": 545
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.34433484077453613,
      "learning_rate": 2.5181818181818183e-05,
      "loss": 5.0968,
      "step": 550
    },
    {
      "epoch": 5.045766590389016,
      "grad_norm": 0.5587162971496582,
      "learning_rate": 2.4954545454545455e-05,
      "loss": 5.002,
      "step": 555
    },
    {
      "epoch": 5.091533180778032,
      "grad_norm": 0.251235693693161,
      "learning_rate": 2.472727272727273e-05,
      "loss": 4.8127,
      "step": 560
    },
    {
      "epoch": 5.137299771167048,
      "grad_norm": 0.41209718585014343,
      "learning_rate": 2.45e-05,
      "loss": 5.3821,
      "step": 565
    },
    {
      "epoch": 5.183066361556064,
      "grad_norm": 0.22817020118236542,
      "learning_rate": 2.4272727272727275e-05,
      "loss": 4.7987,
      "step": 570
    },
    {
      "epoch": 5.22883295194508,
      "grad_norm": 0.40010523796081543,
      "learning_rate": 2.4045454545454547e-05,
      "loss": 5.1782,
      "step": 575
    },
    {
      "epoch": 5.274599542334096,
      "grad_norm": 0.3346295654773712,
      "learning_rate": 2.381818181818182e-05,
      "loss": 4.8392,
      "step": 580
    },
    {
      "epoch": 5.320366132723112,
      "grad_norm": 0.33099907636642456,
      "learning_rate": 2.359090909090909e-05,
      "loss": 5.3509,
      "step": 585
    },
    {
      "epoch": 5.366132723112128,
      "grad_norm": 0.35619762539863586,
      "learning_rate": 2.3363636363636367e-05,
      "loss": 4.7637,
      "step": 590
    },
    {
      "epoch": 5.411899313501144,
      "grad_norm": 0.44672802090644836,
      "learning_rate": 2.3136363636363635e-05,
      "loss": 4.3733,
      "step": 595
    },
    {
      "epoch": 5.4576659038901605,
      "grad_norm": 0.33248448371887207,
      "learning_rate": 2.290909090909091e-05,
      "loss": 4.527,
      "step": 600
    },
    {
      "epoch": 5.503432494279176,
      "grad_norm": 0.5877517461776733,
      "learning_rate": 2.2681818181818183e-05,
      "loss": 5.2945,
      "step": 605
    },
    {
      "epoch": 5.549199084668192,
      "grad_norm": 0.2748199999332428,
      "learning_rate": 2.2454545454545455e-05,
      "loss": 4.3355,
      "step": 610
    },
    {
      "epoch": 5.5949656750572085,
      "grad_norm": 0.35936516523361206,
      "learning_rate": 2.2227272727272727e-05,
      "loss": 4.6175,
      "step": 615
    },
    {
      "epoch": 5.640732265446224,
      "grad_norm": 0.4519258737564087,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 5.1084,
      "step": 620
    },
    {
      "epoch": 5.68649885583524,
      "grad_norm": 0.2844758927822113,
      "learning_rate": 2.177272727272727e-05,
      "loss": 5.0077,
      "step": 625
    },
    {
      "epoch": 5.732265446224257,
      "grad_norm": 0.44664013385772705,
      "learning_rate": 2.1545454545454547e-05,
      "loss": 4.6279,
      "step": 630
    },
    {
      "epoch": 5.778032036613272,
      "grad_norm": 0.33750104904174805,
      "learning_rate": 2.131818181818182e-05,
      "loss": 4.3076,
      "step": 635
    },
    {
      "epoch": 5.823798627002288,
      "grad_norm": 0.41449159383773804,
      "learning_rate": 2.109090909090909e-05,
      "loss": 4.9266,
      "step": 640
    },
    {
      "epoch": 5.869565217391305,
      "grad_norm": 0.3147633969783783,
      "learning_rate": 2.0863636363636367e-05,
      "loss": 4.7835,
      "step": 645
    },
    {
      "epoch": 5.91533180778032,
      "grad_norm": 0.32130807638168335,
      "learning_rate": 2.0636363636363636e-05,
      "loss": 4.3356,
      "step": 650
    },
    {
      "epoch": 5.961098398169336,
      "grad_norm": 0.3029286861419678,
      "learning_rate": 2.040909090909091e-05,
      "loss": 4.8315,
      "step": 655
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.43569236993789673,
      "learning_rate": 2.0181818181818183e-05,
      "loss": 5.3377,
      "step": 660
    },
    {
      "epoch": 6.045766590389016,
      "grad_norm": 0.3031725585460663,
      "learning_rate": 1.9954545454545455e-05,
      "loss": 5.1181,
      "step": 665
    },
    {
      "epoch": 6.091533180778032,
      "grad_norm": 0.6231600642204285,
      "learning_rate": 1.9727272727272728e-05,
      "loss": 4.8397,
      "step": 670
    },
    {
      "epoch": 6.137299771167048,
      "grad_norm": 0.24869661033153534,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 4.4648,
      "step": 675
    },
    {
      "epoch": 6.183066361556064,
      "grad_norm": 0.28508061170578003,
      "learning_rate": 1.9272727272727272e-05,
      "loss": 4.6464,
      "step": 680
    },
    {
      "epoch": 6.22883295194508,
      "grad_norm": 0.24767448008060455,
      "learning_rate": 1.9045454545454547e-05,
      "loss": 4.6848,
      "step": 685
    },
    {
      "epoch": 6.274599542334096,
      "grad_norm": 0.7914524078369141,
      "learning_rate": 1.881818181818182e-05,
      "loss": 5.2494,
      "step": 690
    },
    {
      "epoch": 6.320366132723112,
      "grad_norm": 0.5356543660163879,
      "learning_rate": 1.859090909090909e-05,
      "loss": 4.8405,
      "step": 695
    },
    {
      "epoch": 6.366132723112128,
      "grad_norm": 0.23406575620174408,
      "learning_rate": 1.8363636363636364e-05,
      "loss": 4.3756,
      "step": 700
    },
    {
      "epoch": 6.411899313501144,
      "grad_norm": 0.4499486982822418,
      "learning_rate": 1.8136363636363636e-05,
      "loss": 4.5714,
      "step": 705
    },
    {
      "epoch": 6.4576659038901605,
      "grad_norm": 0.2599621117115021,
      "learning_rate": 1.7909090909090908e-05,
      "loss": 4.9026,
      "step": 710
    },
    {
      "epoch": 6.503432494279176,
      "grad_norm": 0.3017932176589966,
      "learning_rate": 1.7681818181818183e-05,
      "loss": 4.4709,
      "step": 715
    },
    {
      "epoch": 6.549199084668192,
      "grad_norm": 0.2266877442598343,
      "learning_rate": 1.7454545454545456e-05,
      "loss": 4.5212,
      "step": 720
    },
    {
      "epoch": 6.5949656750572085,
      "grad_norm": 0.25813859701156616,
      "learning_rate": 1.7227272727272728e-05,
      "loss": 5.0349,
      "step": 725
    },
    {
      "epoch": 6.640732265446224,
      "grad_norm": 0.2528627812862396,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 4.2514,
      "step": 730
    },
    {
      "epoch": 6.68649885583524,
      "grad_norm": 0.3091779351234436,
      "learning_rate": 1.6772727272727272e-05,
      "loss": 4.8156,
      "step": 735
    },
    {
      "epoch": 6.732265446224257,
      "grad_norm": 0.27561432123184204,
      "learning_rate": 1.6545454545454548e-05,
      "loss": 4.829,
      "step": 740
    },
    {
      "epoch": 6.778032036613272,
      "grad_norm": 0.29036542773246765,
      "learning_rate": 1.631818181818182e-05,
      "loss": 5.0068,
      "step": 745
    },
    {
      "epoch": 6.823798627002288,
      "grad_norm": 0.6991926431655884,
      "learning_rate": 1.6090909090909092e-05,
      "loss": 5.6433,
      "step": 750
    },
    {
      "epoch": 6.869565217391305,
      "grad_norm": 0.3101561367511749,
      "learning_rate": 1.5863636363636364e-05,
      "loss": 4.87,
      "step": 755
    },
    {
      "epoch": 6.91533180778032,
      "grad_norm": 0.18647047877311707,
      "learning_rate": 1.563636363636364e-05,
      "loss": 4.5832,
      "step": 760
    },
    {
      "epoch": 6.961098398169336,
      "grad_norm": 0.2973349392414093,
      "learning_rate": 1.5409090909090908e-05,
      "loss": 4.2686,
      "step": 765
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.2908526062965393,
      "learning_rate": 1.5181818181818184e-05,
      "loss": 4.677,
      "step": 770
    },
    {
      "epoch": 7.045766590389016,
      "grad_norm": 0.34619998931884766,
      "learning_rate": 1.4954545454545454e-05,
      "loss": 4.6075,
      "step": 775
    },
    {
      "epoch": 7.091533180778032,
      "grad_norm": 0.2532341480255127,
      "learning_rate": 1.4727272727272728e-05,
      "loss": 3.9641,
      "step": 780
    },
    {
      "epoch": 7.137299771167048,
      "grad_norm": 0.24433061480522156,
      "learning_rate": 1.45e-05,
      "loss": 4.6605,
      "step": 785
    },
    {
      "epoch": 7.183066361556064,
      "grad_norm": 0.20969028770923615,
      "learning_rate": 1.4272727272727274e-05,
      "loss": 4.3738,
      "step": 790
    },
    {
      "epoch": 7.22883295194508,
      "grad_norm": 0.20224200189113617,
      "learning_rate": 1.4045454545454544e-05,
      "loss": 4.9644,
      "step": 795
    },
    {
      "epoch": 7.274599542334096,
      "grad_norm": 0.34237954020500183,
      "learning_rate": 1.3818181818181818e-05,
      "loss": 5.0742,
      "step": 800
    },
    {
      "epoch": 7.320366132723112,
      "grad_norm": 0.20083406567573547,
      "learning_rate": 1.359090909090909e-05,
      "loss": 4.7794,
      "step": 805
    },
    {
      "epoch": 7.366132723112128,
      "grad_norm": 0.2760472297668457,
      "learning_rate": 1.3363636363636364e-05,
      "loss": 4.478,
      "step": 810
    },
    {
      "epoch": 7.411899313501144,
      "grad_norm": 0.20306695997714996,
      "learning_rate": 1.3136363636363638e-05,
      "loss": 4.3017,
      "step": 815
    },
    {
      "epoch": 7.4576659038901605,
      "grad_norm": 0.31508782505989075,
      "learning_rate": 1.290909090909091e-05,
      "loss": 4.4378,
      "step": 820
    },
    {
      "epoch": 7.503432494279176,
      "grad_norm": 0.2581658959388733,
      "learning_rate": 1.2681818181818184e-05,
      "loss": 4.799,
      "step": 825
    },
    {
      "epoch": 7.549199084668192,
      "grad_norm": 0.28180640935897827,
      "learning_rate": 1.2454545454545454e-05,
      "loss": 4.4218,
      "step": 830
    },
    {
      "epoch": 7.5949656750572085,
      "grad_norm": 0.3013913631439209,
      "learning_rate": 1.2227272727272728e-05,
      "loss": 5.6864,
      "step": 835
    },
    {
      "epoch": 7.640732265446224,
      "grad_norm": 0.4113348722457886,
      "learning_rate": 1.2e-05,
      "loss": 4.7655,
      "step": 840
    },
    {
      "epoch": 7.68649885583524,
      "grad_norm": 0.42480307817459106,
      "learning_rate": 1.1772727272727272e-05,
      "loss": 4.7027,
      "step": 845
    },
    {
      "epoch": 7.732265446224257,
      "grad_norm": 0.26968273520469666,
      "learning_rate": 1.1545454545454545e-05,
      "loss": 4.7986,
      "step": 850
    },
    {
      "epoch": 7.778032036613272,
      "grad_norm": 0.29305168986320496,
      "learning_rate": 1.1318181818181818e-05,
      "loss": 4.7922,
      "step": 855
    },
    {
      "epoch": 7.823798627002288,
      "grad_norm": 0.2787967324256897,
      "learning_rate": 1.1090909090909092e-05,
      "loss": 5.0338,
      "step": 860
    },
    {
      "epoch": 7.869565217391305,
      "grad_norm": 0.4353559613227844,
      "learning_rate": 1.0863636363636364e-05,
      "loss": 5.0698,
      "step": 865
    },
    {
      "epoch": 7.91533180778032,
      "grad_norm": 0.22069187462329865,
      "learning_rate": 1.0636363636363638e-05,
      "loss": 4.3805,
      "step": 870
    },
    {
      "epoch": 7.961098398169336,
      "grad_norm": 0.4298684895038605,
      "learning_rate": 1.040909090909091e-05,
      "loss": 5.2535,
      "step": 875
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.30257341265678406,
      "learning_rate": 1.0181818181818182e-05,
      "loss": 5.0334,
      "step": 880
    },
    {
      "epoch": 8.045766590389016,
      "grad_norm": 0.30473092198371887,
      "learning_rate": 9.954545454545455e-06,
      "loss": 4.4003,
      "step": 885
    },
    {
      "epoch": 8.091533180778033,
      "grad_norm": 0.24293550848960876,
      "learning_rate": 9.727272727272728e-06,
      "loss": 4.6088,
      "step": 890
    },
    {
      "epoch": 8.137299771167047,
      "grad_norm": 0.1997694969177246,
      "learning_rate": 9.5e-06,
      "loss": 4.7271,
      "step": 895
    },
    {
      "epoch": 8.183066361556063,
      "grad_norm": 0.20650474727153778,
      "learning_rate": 9.272727272727273e-06,
      "loss": 5.0179,
      "step": 900
    },
    {
      "epoch": 8.22883295194508,
      "grad_norm": 0.4768315553665161,
      "learning_rate": 9.045454545454546e-06,
      "loss": 4.5988,
      "step": 905
    },
    {
      "epoch": 8.274599542334096,
      "grad_norm": 0.23728473484516144,
      "learning_rate": 8.818181818181819e-06,
      "loss": 5.4946,
      "step": 910
    },
    {
      "epoch": 8.320366132723112,
      "grad_norm": 0.3424108922481537,
      "learning_rate": 8.59090909090909e-06,
      "loss": 4.9288,
      "step": 915
    },
    {
      "epoch": 8.366132723112129,
      "grad_norm": 0.31052911281585693,
      "learning_rate": 8.363636363636365e-06,
      "loss": 4.5625,
      "step": 920
    },
    {
      "epoch": 8.411899313501145,
      "grad_norm": 0.2898115813732147,
      "learning_rate": 8.136363636363637e-06,
      "loss": 4.6258,
      "step": 925
    },
    {
      "epoch": 8.45766590389016,
      "grad_norm": 0.5061244964599609,
      "learning_rate": 7.909090909090909e-06,
      "loss": 5.1732,
      "step": 930
    },
    {
      "epoch": 8.503432494279176,
      "grad_norm": 0.4666973352432251,
      "learning_rate": 7.681818181818181e-06,
      "loss": 4.6078,
      "step": 935
    },
    {
      "epoch": 8.549199084668192,
      "grad_norm": 0.24860671162605286,
      "learning_rate": 7.454545454545454e-06,
      "loss": 5.2189,
      "step": 940
    },
    {
      "epoch": 8.594965675057209,
      "grad_norm": 0.23229406774044037,
      "learning_rate": 7.227272727272727e-06,
      "loss": 4.5822,
      "step": 945
    },
    {
      "epoch": 8.640732265446225,
      "grad_norm": 0.5587146282196045,
      "learning_rate": 7.000000000000001e-06,
      "loss": 4.1814,
      "step": 950
    },
    {
      "epoch": 8.68649885583524,
      "grad_norm": 0.3252582550048828,
      "learning_rate": 6.772727272727274e-06,
      "loss": 4.4728,
      "step": 955
    },
    {
      "epoch": 8.732265446224256,
      "grad_norm": 0.23646149039268494,
      "learning_rate": 6.545454545454547e-06,
      "loss": 4.3521,
      "step": 960
    },
    {
      "epoch": 8.778032036613272,
      "grad_norm": 0.2126302719116211,
      "learning_rate": 6.318181818181819e-06,
      "loss": 4.2742,
      "step": 965
    },
    {
      "epoch": 8.823798627002288,
      "grad_norm": 0.2239774763584137,
      "learning_rate": 6.090909090909091e-06,
      "loss": 4.8999,
      "step": 970
    },
    {
      "epoch": 8.869565217391305,
      "grad_norm": 0.49575281143188477,
      "learning_rate": 5.863636363636364e-06,
      "loss": 4.4207,
      "step": 975
    },
    {
      "epoch": 8.915331807780321,
      "grad_norm": 0.2761962413787842,
      "learning_rate": 5.636363636363637e-06,
      "loss": 4.9919,
      "step": 980
    },
    {
      "epoch": 8.961098398169337,
      "grad_norm": 0.23673081398010254,
      "learning_rate": 5.40909090909091e-06,
      "loss": 5.0065,
      "step": 985
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.4237029552459717,
      "learning_rate": 5.181818181818182e-06,
      "loss": 4.7034,
      "step": 990
    },
    {
      "epoch": 9.045766590389016,
      "grad_norm": 0.25349879264831543,
      "learning_rate": 4.954545454545455e-06,
      "loss": 4.7819,
      "step": 995
    },
    {
      "epoch": 9.091533180778033,
      "grad_norm": 0.22058115899562836,
      "learning_rate": 4.727272727272727e-06,
      "loss": 4.8851,
      "step": 1000
    }
  ],
  "logging_steps": 5,
  "max_steps": 1100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8654943045550080.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
